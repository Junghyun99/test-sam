**beads-mcp**는 Beads를 Anthropic이 발표한 **MCP(Model Context Protocol)** 표준에 맞춰 사용할 수 있도록 해주는 **연결 커넥터(Bridge)**입니다.

이것을 이해하려면 먼저 **MCP**가 무엇인지 알아야 합니다.

---

### 1. MCP(Model Context Protocol)란?
MCP는 Claude와 같은 AI 모델이 **내 컴퓨터의 로컬 데이터나 도구(파일, 데이터베이스, CLI 등)에 안전하고 표준화된 방식으로 접근**할 수 있도록 만든 통규 규약입니다. 

기존에는 AI에게 "내 파일을 읽어봐"라고 하려면 복잡한 설정이 필요했지만, MCP를 사용하면 AI가 마치 내 컴퓨터의 기능을 **'플러그인'**처럼 자유자재로 쓸 수 있습니다.

### 2. beads-mcp는 무엇을 하나요?
`beads-mcp`는 Beads의 기능들을 MCP 서버 형태로 제공합니다. 이를 통해 Claude Desktop이나 MCP를 지원하는 다른 AI 에이전트들이 **Beads를 훨씬 더 "직접적"으로** 다룰 수 있게 됩니다.

*   **CLI 방식 (그냥 Beads):** AI가 터미널에 `bd list`라고 직접 타이핑하고 그 텍스트 결과를 읽어야 합니다. (중간에 오타가 나거나 파싱 에러가 날 확률이 있음)
*   **MCP 방식 (beads-mcp):** AI의 인터페이스 안에 `beads_create_task`, `beads_list_ready` 같은 **전용 버튼(함수)**이 생기는 것과 같습니다. AI가 명령어를 타이핑할 필요 없이, 미리 정의된 기능을 직접 호출하므로 훨씬 빠르고 정확합니다.

### 3. beads-mcp를 쓰면 뭐가 좋아지나요?

1.  **설정이 매우 간편해집니다:**
    Claude Desktop 설정 파일에 `beads-mcp` 정보만 넣어두면, Claude가 실행될 때마다 자동으로 "아, 나는 오늘부터 Beads라는 도구를 쓸 수 있구나!"라고 바로 인지합니다.
2.  **더 강력한 컨텍스트 제공:**
    단순히 텍스트를 주고받는 것을 넘어, MCP를 통해 현재 프로젝트의 태스크 상태를 AI에게 더 정교하게 전달할 수 있습니다.
3.  **에이전트와의 밀착 연동:**
    예를 들어 Anthropic의 **Claude Desktop**이나 **Cursor** 같은 도구에서 Beads를 사용할 때, AI가 마치 원래 그 도구의 일부인 것처럼 자연스럽게 작업 계획을 세우고 관리합니다.

### 4. 질문자님(로컬 LLM 사용자)에게 주는 의미
만약 질문자님이 **MCP를 지원하는 에이전트 소프트웨어**를 쓰고 계신다면, `beads-mcp`를 사용하는 것이 그냥 CLI 버전(`bd`)만 쓰는 것보다 훨씬 편리합니다.

*   **준비물:** `bd` 바이너리 + `beads-mcp` 서버 프로그램
*   **작동:** 로컬 LLM이 "작업 목록 보여줘"라는 말을 들으면, 터미널 명령어를 고민하는 대신 `beads-mcp` 서버에 정의된 기능을 즉시 실행합니다.

### 요약
*   **Beads:** 실제 작업을 기록하고 관리하는 **핵심 도구(Engine)**.
*   **beads-mcp:** 그 도구를 Claude나 다른 AI 에이전트들이 **쉽게 사용할 수 있도록 연결해주는 통로(Adapter)**.

최근 AI 코딩 도구들이 MCP 표준을 대거 채택하고 있기 때문에, Steve Yegge도 Beads가 널리 쓰이도록 이 MCP 버전을 함께 내놓은 것입니다. 만약 Claude Desktop 등을 사용하신다면 `beads-mcp`를 설치하는 것이 가장 세련된 사용 방법입니다.



Beads를 LLM에게 인식시키기 위해 수동으로 작성된 긴 MD 파일을 찾으실 필요는 없습니다. 제작자인 Steve Yegge는 이 과정을 자동화하기 위해 **`bd onboard`**라는 아주 편리한 명령어를 만들어 두었습니다.

결론부터 말씀드리면, **`bd onboard` 명령어를 실행했을 때 터미널에 출력되는 Markdown 내용을 복사해서 `AGENTS.md`나 `CLAUDE.md` 파일에 넣어주시면 됩니다.**

### 1. 시스템 프롬프트 설정 방법 (워크플로우)

1.  **터미널에서 명령어 실행:**
    ```bash
    bd onboard
    ```
    이 명령어를 실행하면 AI 에이전트(LLM)가 읽어야 할 **최적화된 사용 가이드라인(Markdown)**이 출력됩니다.

2.  **파일 생성 및 붙여넣기:**
    프로젝트 루트 디렉토리에 **`AGENTS.md`** 또는 **`CLAUDE.md`** (Claude Code 사용 시) 파일을 만들고, 위에서 나온 내용을 그대로 붙여넣으세요.

3.  **LLM에게 지시:**
    LLM에게 다음과 같이 한 마디만 하면 됩니다.
    > "이 프로젝트의 `AGENTS.md` 파일을 읽고, 앞으로 모든 작업 계획과 진행 상태는 `bd` 도구를 사용해서 관리해줘."

---

### 2. 그 MD 파일(지침)에는 어떤 내용이 들어있나요?

`bd onboard`가 생성하는 지침에는 LLM이 실수하지 않도록 다음과 같은 **"AI 전용 수칙"**이 포함되어 있습니다.

*   **기본 원칙:** "모든 할 일은 Markdown 파일이 아닌 `bd` 명령어로 관리할 것."
*   **세션 시작 시:** `bd ready`를 실행하여 현재 즉시 착수 가능한(의존성이 해결된) 작업이 무엇인지 확인할 것.
*   **새로운 발견 시:** 코딩 중 버그나 추가 할 일을 발견하면 즉시 `bd create`로 티켓을 생성하고 현재 작업과 연결(`dep add`)할 것.
*   **세션 종료 시:** 작업 내용을 업데이트하고 `bd sync`를 통해 Git에 반영할 것.

---

### 3. 왜 파일로 주는 것이 좋은가요?

*   **토큰 절약:** 매번 채팅창에 길게 설명할 필요가 없습니다. 파일에 적어두면 에이전트가 필요할 때만 참조하므로 효율적입니다.
*   **일관성:** 어떤 LLM(Claude, GPT, 로컬 Llama 등)을 쓰더라도 동일한 규칙에 따라 작업을 관리하게 됩니다.
*   **자동 업데이트:** `bd` 도구가 업데이트되어 기능이 추가되면, 다시 `bd onboard`를 실행해 파일 내용만 갱신해주면 됩니다.

### 요약
따로 파일을 찾으러 다니지 마시고, 설치된 `bd` CLI에서 **`bd onboard`**를 입력하세요. 거기서 나오는 텍스트가 바로 **가장 완벽한 시스템 프롬프트**입니다. 이 내용을 `AGENTS.md`에 저장해두면, 로컬 LLM이 그 파일을 읽고 스스로 Beads 전문가가 되어 당신의 프로젝트를 관리하기 시작할 것입니다.


기능 요구사항이라는 추상적인 아이디어를 **AI가 즉시 실행 가능한 구체적인 Task(Beads)**로 바꾸는 과정은 일종의 **'설계 공정'**과 같습니다.

이를 위해 가장 권장되는 **4단계 워크플로우**를 제안해 드립니다. 이 과정을 거치면 AI가 엉뚱한 코드를 짜는 일을 획기적으로 줄일 수 있습니다.

---

### 1단계: Spec-First (문서화 단계)
무작정 `bd create`를 하기 전에, 먼저 요구사항을 상세히 적은 **`SPEC.md`** 파일을 작성하게 하세요.

*   **방법:** 로컬 LLM(Architect 모드)에게 이렇게 시킵니다.
    > "사용자 로그인 기능을 만들 거야. 이 기능에 대한 `SPEC.md`를 작성해줘. 여기에는 **사용자 스토리, 데이터베이스 스키마, API 엔드포인트 정의, 그리고 발생 가능한 에러 케이스(Edge Cases)**가 포함되어야 해."
*   **이유:** 문서화 과정에서 LLM은 스스로 논리적 모순을 발견하고, 구체적인 구현 방법(어떤 라이브러리를 쓸지, 변수명은 뭘로 할지 등)을 미리 결정하게 됩니다.

### 2단계: 사람의 리뷰 및 확정
AI가 만든 `SPEC.md`를 질문자님이 직접 읽어봅니다.
*   "아, 이 부분은 보안상 이렇게 하면 안 돼." 또는 "우리 회사 DB 구조랑 안 맞네." 같은 부분을 수정합니다.
*   이 단계가 끝나면 **'설계도'**가 완성된 것입니다.

### 3단계: Task Decomposition (작업 뽀개기)
이제 완성된 `SPEC.md`를 바탕으로 작업을 **원자 단위(Atomic Unit)**로 쪼개야 합니다.

*   **LLM에게 주는 프롬프트:**
    > "이제 확정된 `SPEC.md`를 바탕으로 개발 작업을 쪼개줘. 각 작업은 **'30분 이내에 끝낼 수 있는 단위'**여야 하고, `bd` 명령어를 사용해서 생성해줘. 작업 간의 의존성(Dependency)도 반드시 고려해서 `bd dep add` 명령어를 포함해줘."
*   **쪼개는 기준 (팁):**
    1.  **DB/인프라:** 테이블 생성, 환경 변수 설정.
    2.  **데이터 모델:** Entity/DTO 클래스 작성.
    3.  **핵심 로직:** 비즈니스 로직(Service 레이어) 구현.
    4.  **외부 연결:** API 컨트롤러, 외부 API 연동.
    5.  **검증:** 단위 테스트(Unit Test) 작성.
    6.  **UI/UX:** 프론트엔드 작업.

### 4단계: Beads에 등록 (Registry)
LLM이 생성한 `bd` 명령어들을 실행하여 `.beads/`에 저장합니다. 이때 Beads의 **의존성 그래프**가 빛을 발합니다.

**예시 결과물:**
```bash
# 1. 큰 목표(에픽) 생성
bd create "사용자 로그인 기능 구현" --epic (ID: e1)

# 2. 세부 작업 생성 및 의존성 연결
bd create "Users 테이블 마이그레이션 파일 작성" (ID: t1)
bd dep add t1 e1

bd create "비밀번호 해싱 로직 구현" (ID: t2)
bd dep add t2 t1  # 테이블이 있어야 로직을 짤 수 있음

bd create "로그인 API 엔드포인트 구현" (ID: t3)
bd dep add t3 t2  # 해싱 로직이 있어야 API를 완성함

bd create "로그인 기능 단위 테스트 작성" (ID: t4)
bd dep add t4 t3  # API가 나와야 테스트를 함
```

---

### 이 방식이 강력한 이유

1.  **AI가 길을 잃지 않음:** AI는 `bd ready`를 칠 때마다 **"지금 내가 할 수 있는 유일한 작업"**인 `t1`만 보게 됩니다. 작업이 꼬일 일이 없습니다.
2.  **맥락 유지:** 나중에 코딩을 시작할 때 AI에게 "`t2` 작업을 시작해"라고 하면, AI는 이미 연결된 `SPEC.md`와 이전 작업(`t1`)의 결과물을 참고하여 정확한 코드를 짭니다.
3.  **중단 및 재개 용이:** 퇴근했다가 다음 날 돌아와도 `bd list`만 치면 어디까지 설계되었고 어디서부터 코딩하면 되는지 즉시 파악됩니다.

### 요약: 질문자님이 하실 일
1.  **아이디어 던지기** (로컬 LLM에게)
2.  **SPEC.md 리뷰** (사람이 직접)
3.  **"자, 이제 이걸 `bd` 티켓으로 쪼개줘"** 라고 명령하기
4.  **`bd ready`**로 하나씩 코딩 진행시키기

이렇게 **'문서화 -> 티켓팅 -> 구현'**의 단계를 거치는 것이 AI와 협업할 때 가장 디테일한 설계를 유지하는 비결입니다.